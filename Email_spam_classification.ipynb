{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Email spam classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_REHQPQYFsCM",
        "outputId": "87d98958-1ff3-4918-f006-24f203d017dd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(text) # create a vocabulary from text input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textual data  must be converted to numerical format This is achieved by tokenizing the text i.e. parsing individual words from the text and then assigning a numerical value for each word. In simplest form this is the word count or frequency. This approach does not pay any attention to order in which words appear in the text; only count and frequency is stored."
      ],
      "metadata": {
        "id": "ZPgZCb1TGakM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab=\",vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGxMaP8wGG8T",
        "outputId": "6779e961-1b46-4509-8c14-3be3d8df3aae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab= {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizer.transform(text)\n",
        "#Now any text can be encoded in numerical vector as follows"
      ],
      "metadata": {
        "id": "HlmdBxVoGLLX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing the contents of vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRqG60jnG0-v",
        "outputId": "6200ff70-f31c-4b9f-f77e-b30723cb5cb2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector = vectorizer.transform([\"the brown fox and big puppy\"])\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdTcQOXlG3wl",
        "outputId": "51582953-b46c-4436-fc69-123dbfee6882"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8)\n",
            "[[1 0 1 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encoding can be done for several pieces of text at the same time.\n",
        "vector = vectorizer.transform([\"the brown fox\",\"lazy dog jumped\"])\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGMc60BYG60z",
        "outputId": "2643fec6-ee7c-4af6-c81f-3c172a4e8a30"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 8)\n",
            "[[1 0 1 0 0 0 0 1]\n",
            " [0 1 0 1 1 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "9JBeX1chG8-9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"emails.csv\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqbVHiNtG_y8",
        "outputId": "647d8b33-4848-45e0-f207-4ea83e8d9626"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  spam\n",
            "0  Subject: naturally irresistible your corporate...     1\n",
            "1  Subject: the stock trading gunslinger  fanny i...     1\n",
            "2  Subject: unbelievable new homes made easy  im ...     1\n",
            "3  Subject: 4 color printing special  request add...     1\n",
            "4  Subject: do not have money , get software cds ...     1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.spam.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6rt8SpIHRx8",
        "outputId": "e7380a8f-3efb-406d-d68c-ab6b58eeed85"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    4360\n",
            "1    1368\n",
            "Name: spam, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"text\"]\n",
        "y = df[\"spam\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=10)\n",
        "print(X_test)\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMhQWAiRINI4",
        "outputId": "e0d14a63-a312-4b3f-ea50-7f23bae55cb3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2507    Subject: tentative schedule of the talks at si...\n",
            "3886    Subject: re : thanks !  karin ,  i talked to m...\n",
            "3330    Subject: approval for restricted websit : web ...\n",
            "5613    Subject: re : full version  i read the chapter...\n",
            "2357    Subject: spreadsheet for george posey  vince a...\n",
            "                              ...                        \n",
            "1642    Subject: christie and vince :  on behalf of en...\n",
            "2255    Subject: e & p company model  mark ,  did you ...\n",
            "753     Subject: affordable - the way medications shou...\n",
            "2263                          Subject: elena chilkina  hi\n",
            "282     Subject: prime lenders application status  we ...\n",
            "Name: text, Length: 1146, dtype: object\n",
            "2507    0\n",
            "3886    0\n",
            "3330    0\n",
            "5613    0\n",
            "2357    0\n",
            "       ..\n",
            "1642    0\n",
            "2255    0\n",
            "753     1\n",
            "2263    0\n",
            "282     1\n",
            "Name: spam, Length: 1146, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Using english language stop words ignores common words like \"the\", \"and\" etc. from vocabulary.\n",
        "vect = CountVectorizer(stop_words=\"english\")\n",
        "vect.fit(X_train)\n",
        "X_train_df = vect.transform(X_train)\n",
        "X_test_df = vect.transform(X_test)"
      ],
      "metadata": {
        "id": "aHmT4lgZQLmq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model building, training, prediction and evaluation is the same before now that we have our data in correct form\n",
        "model = svm.SVC()\n",
        "model.fit(X_train_df,y_train)\n",
        "y_pred = model.predict(X_test_df)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred,target_names=[\"not spam\",\"spam\"]))"
      ],
      "metadata": {
        "id": "zYPf7MYvQN59",
        "outputId": "7dafcd03-927c-4136-ee76-e8cec666916d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[859   2]\n",
            " [ 24 261]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    not spam       0.97      1.00      0.99       861\n",
            "        spam       0.99      0.92      0.95       285\n",
            "\n",
            "    accuracy                           0.98      1146\n",
            "   macro avg       0.98      0.96      0.97      1146\n",
            "weighted avg       0.98      0.98      0.98      1146\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For extra verification and peace of mind we look at some correctly predicted samples of spam and ham\n",
        "y_test2 = np.array(y_test)\n",
        "y_pred2 = np.array(y_pred)\n",
        "\n",
        "idx = np.logical_and(y_pred2 == 0, y_test2 == 0)\n",
        "spam0 = X_test[idx]\n",
        "print(\"Not spam: \",np.array(spam0.index))\n",
        "print(\"Not spam sample=\",X_test[3886])\n",
        "print(\"Not spam sample=\",X_test[5613])\n",
        "\n",
        "\n",
        "idx = np.logical_and(y_pred2 == 1, y_test2 == 1)\n",
        "spam = X_test[idx]\n",
        "print(\"spam: \",np.array(spam.index))\n",
        "print(\"spam sample=\",X_test[282])\n",
        "print(\"spam sample=\",X_test[225])"
      ],
      "metadata": {
        "id": "MoP8FQiHQP2G",
        "outputId": "6a604c0c-789e-42af-89ec-59bd794721f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not spam:  [2507 3886 3330 5613 2357 5547 3169 3471 1858 4788 3643 3539 5460 2943\n",
            " 3328 5032 3147 2637 1710 5443 2310 5466 2813 4156 5141 1521 2863 2961\n",
            " 2484 3038 4368 1598 4607 3637 2492 5591 3399 3758 3035 5652 4629 5474\n",
            " 3670 2329 3977 1429 3205 4151 2722 4718 5521 3908 5693 3295 5045 3905\n",
            " 4227 3121 1899 1563 3561 3459 4762 5595 4549 4831 4006 4418 2569 5001\n",
            " 3962 5026 5284 2224 3774 1622 2716 1486 4489 3563 4135 4744 5073 4106\n",
            " 2178 1510 1492 1413 4340 5290 3283 2709 5038 3248 3916 5556 2309 1528\n",
            " 5077 3805 5059 5705 3186 4828 5100 4386 4552 1773 3347 4494 4478 3880\n",
            " 3130 1874 4943 4326 4935 2897 2473 5179 4408 3304 4427 3752 4591 3769\n",
            " 3777 3982 1634 5004 2497 3723 4189 5436 3357 4152 3233 4415 2020 4915\n",
            " 2214 3581 5467 2397 3297 2393 4647 3852 5583 2173 4221 4564 3476 2136\n",
            " 5003 2970 5464 1796 1766 4067 5692 1735 1729 3853 5544 5684 5355 1437\n",
            " 4343 2779 4298 3829 3114 3759 4339 3760 3060 5171 4020 2108 4114 5212\n",
            " 3043 1412 2820 4270 3607 3698 1446 3809 4479 1628 1484 3264 2367 4673\n",
            " 3245 1841 2794 5380 2792 3438 4824 2772 3793 2150 2879 2656 1615 4516\n",
            " 2891 3134 1997 1846 5478 1638 2705 2121 4996 2628 4512 3099 4565 5700\n",
            " 4351 4641 2294 2587 2101 4120 5292 2829 4027 4941 3555 2831 5195 3410\n",
            " 1630 4419 5197 5573 3436 5131 2939 5317 2543 5294 5163 2468 2026 3325\n",
            " 4350 3703 3467 3343 2179 3456 4493 2589 3378 3764 1534 2264 1369 3713\n",
            " 5304 2480 3838 4239 5207 2502 3057 3430 5578 3940 5143 5598 5433 5230\n",
            " 5396 1576 3600 3010 3586 3661 2912 3802 3738 3518 1370 5425 3122 5187\n",
            " 1409 4804 2999 3371 1776 1967 3469 2181 2793 3191 2726 2626 2574 5208\n",
            " 2717 4291 2640 3996 2766 3244 3321 3052 5678 5303 4277 4139 4846 3344\n",
            " 3556 3817 2487 5542 1931 1915 5676 2755 4145 4211 4946 2474 2724 2908\n",
            " 5366 2192 5399 3100 2527 5614 2243 1483 4193 4244 4797 1605 5316 3639\n",
            " 4175 1588 4088 3598 2335 2228 1973 2059 5011 4360 4254 1419 5263 4130\n",
            " 1583 1592 4428 2089 5350 3455 4630 5093 1799 4303 5611 3788 2351 4138\n",
            " 5159 2699 4573 2049 5269 1894 5046 5452 3204 4144 5275 3863 4730 4635\n",
            " 1398 1856 5691 3373 1808 3277 3994 3291 5384 4284 2906 1674 5493 4999\n",
            " 1438 2751 4864 2945 5416 2218 4945 3584 2359 5497 4756 4653 4036 4391\n",
            " 4956 5629 3780 3345 3042 4475 3138 1474 4290 4559 3677 4628 3579 3571\n",
            " 2211 1372 5340 1910 5411 5148 4188 3447 1917 2276 3691 2210 1750 3051\n",
            " 3928 5094 2837 2273 2960 2436 2419 3117 3365 1676 2706 3128 1873 2689\n",
            " 5369 5682 5345 2737 1523 1616 3132 2972 3879 5620 3709 3318 5329 2289\n",
            " 2142 4124 3127 5242 5630 4040 4721 4262 1712 1800 3472 5621 4904 1929\n",
            " 2414 3895 3465 4725 3972 3137 2237 3433 4803 5068 4398 5553 3959 2582\n",
            " 1860 1400 1957 4200 5121 4025 3979 5576 5554 1549 3854 3936 1980 3929\n",
            " 1442 3993 4894 5023 2257 2843 4949 1589 4434 3690 3913 4658 3822 2636\n",
            " 3372 4880 4077 4056 2514 4940 3625 3705 2314 3807 2925 1722 5628 3525\n",
            " 2662 2215 4148 2093 1705 1875 3200 1745 2517 2664 3894 4526 1811 3073\n",
            " 3063 4041 3939 5566 1506 4962 1925 3336 1971 3413 5562 2091 2463 4712\n",
            " 2526 3113 3619 2753 3075 2475 4413 5223 2277 1651 4013 2650 2074 2396\n",
            " 3349 1948 5339 4178 2341 1885 1463 4959 4090 1561 2364 2040 4938 2318\n",
            " 3508 5268 1636 2104 3505 4593 4800 4095 3172 3583 3466 2973 1373 3926\n",
            " 2904 1427 3143 2180 1768 5567 4625 2594 3338 2785 3383 4045 2603 3153\n",
            " 2249 2415 1789 4080 3363 1656 2327 2353 2913 4399 1464 3247 3492 4595\n",
            " 5549 2380 4245 4215 1401 3958 2489 3715 3499 3675 4848 2389 5561 3564\n",
            " 4383 3285 1654 4825 1852 2660 2598 5146 4185 5525 2949 3403 4892 1999\n",
            " 4438 2966 5421 5018 3209 3515 4851 4979 5357 5149 3202 1550 4217 2736\n",
            " 4169 5130 5005 2546 1383 3694 4276 1657 4533 4249 3727 5022 2747 2439\n",
            " 1733 2764 2411 2470 1444 4724 5589 2584 1838 1877 2138 4129 5298 1983\n",
            " 2377 2174 3696 5119 1637 4619 5168 3679 4643 2460 1731 1976 4626 4793\n",
            " 5498 4977 4424 2992 3621 3139 1843 2809 2233 2326 1451 2133 2964 4714\n",
            " 3530 5044 1797 3945 2370 1940 1988 4274 5608 2094 2153 1807 4646 5170\n",
            " 3867 5361 1459 3588 4273 2453 3941 1562 4554 3893 4883 1478 3324 4226\n",
            " 4879 5138 2046 2167 3251 1938 3222 4807 4121 1380 5113 4808 3097 3956\n",
            " 3821 2260 2485 2996 4081 4889 4439 2458 5035 1601 3398 2006 5432 5107\n",
            " 1663 5458 3947 5671 2184 2595 1692 5261 2320 1961 2668 3229 5615 1707\n",
            " 4372 1572 4960 4180 2787 4076 4539 3717 1906 2773 3198 5509 4816 2613\n",
            " 5660 3254 4874 3227 3606 3674 5517 2282 2878 3592 5232 5413 1702 2557\n",
            " 1962 1683 4365 3551 1986 2554 3224 2840 3077 1945 2280 4317 4614 1963\n",
            " 5286 5217 1642 2255 2263]\n",
            "Not spam sample= Subject: re : thanks !  karin ,  i talked to mike roberts ( the head of the whole weather team ) , and he is  saying that all expenses for tony should be charged to global products team .  this is agreed between vince and jeff shankman .  mike and vince are negotiating with john to put stephen ( or somebody who will  replace him ) to some other cost centres ( via research ) .  it looks like kevin moore is happy if stephen is charged to the same cost  centre as tony .  let us right now charge tony and stephen to the cost centre below .  please , could we charge them separately - when john and vince make their  decision , we should be able to re - charge .  many thanks ,  slava  enron capital & trade resources  canada corp .  from : karin ahamer @ enron 18 / 04 / 2001 15 : 06  to : tani nath / lon / ect @ ect , viacheslav danilov / lon / ect @ ect  cc :  subject : re : thanks !  tani / slava  could you please let me know which costcentre i can bill for any charges  relating to tony and stephen .  thx karin  - - - - - - - - - - - - - - - - - - - - - - forwarded by karin ahamer / eu / enron on 18 / 04 / 2001 15 : 04  - - - - - - - - - - - - - - - - - - - - - - - - - - -  enron capital & trade resources corp .  from : stephen bennett 18 / 04 / 2001 12 : 14  to : karin ahamer / eu / enron @ enron  cc :  subject : re : thanks !  - - - - - - - - - - - - - - - - - - - - - - forwarded by stephen bennett / na / enron on 04 / 18 / 2001  06 : 11 am - - - - - - - - - - - - - - - - - - - - - - - - - - -  kevin g moore @ ect  04 / 18 / 2001 06 : 11 am  to : stephen bennett / na / enron @ enron  cc :  subject : re : thanks !  r . c . 107043  co . # 0413  stephen , all charges for soft ware that you use in london  should be charged to the same cost center as tony hamilton ,  reason being , is that someone will replace you in that position .  thanks  kevin moore  enron north america corp .  from : stephen bennett @ enron 04 / 18 / 2001 05 : 08 am  to : karin ahamer / eu / enron @ enron  cc : kevin g moore / hou / ect @ ect  subject : re : thanks !  you can cost it to my group in houston . kevin moore has the proper number  enron capital & trade resources  canada corp .  from : karin ahamer 04 / 18 / 2001 05 : 06 am  to : kevin g moore / hou / ect @ ect  cc : tony hamilton / eu / enron @ enron , mike a roberts / hou / ect @ ect , stephen  bennett / na / enron @ enron , tani nath / lon / ect @ ect  subject : re : thanks !  kevin  do you know whose costcentre the microsoft frontpage is supposed to go on ?  thx karin  enron capital & trade resources corp .  from : stephen bennett 18 / 04 / 2001 10 : 10  to : karin ahamer / eu / enron @ enron  cc : kevin g moore / hou / ect @ ect , tony hamilton / eu / enron @ enron , mike a  roberts / hou / ect @ ect  subject : thanks !  hi karin . . .  i hope you had a splendid holiday ! i wanted to thank you for getting tony  and i set up here last week . we seem to have established a steady daily  routine and are supporting several different trading groups in london as well  as continuing our daily support of traders in houston . we ' ve gotten far more  requests for information than we expected , so as a result i will be remaining  in london a little longer than originally expected . as of now - i ' m not sure  exactly when i ' ll be going back to houston - but vince has let me know that i  will remain here as long as necessary to ensure adequate daily weather  support for the london traders .  to that end - i think i will need one additional piece of software installed  on the machine that i will be using on a regular basis . could i please get  microsoft frontpage installed as soon as we can get it ?  that leads to the second issue of desks . i know that space is a premium here  - and i understand that i may need to move around some as a result . i  certainly want keep things as simple as possible for everyone - but i also  wanted to make sure that you know that there are certain applications that  are essential for the daily trader support in london and houston . as such ,  if i move , we will need to make sure that these applications are available  from the beginning of the day ( we start about 0600 ) . the applications are :  1 ) adobe acrobat - full version  2 ) accuweather for windows - ( this is something i will need to install )  3 ) microsoft front page  4 ) terminal server  5 ) the full ms office software package  one idea would be to have a pc move with me - that way we would not need to  reinstall this software which could cause problems with the daily support  routine .  thanks again for all of your help . i will let you know - once i know - how  long i will be here . i should hear something from vince over the next few  days giving me an idea .  cheers ,  steve  stephen bennett  senior meteorologist  enron research  temporarily in london : ext 3 - 4761  otherwise : ( 713 ) 345 - 3661\n",
            "Not spam sample= Subject: re : full version  i read the chapter . generally ,  i like it , and find it non - controversial  and well written . of course , this is  a huge topic and there are things  that you haven ' t discussed .  i thought the amount of attention  paid to ols and mle estimation  was a bit overboard , given that you  didn ' t really exploit it in your  results . if you were to state  the likelihood function for  the garch model , it might justify  the amount of attention to  gave to estimation for  simple iid processes , which could then  be thought of as a simple lead - in  and intuition builder to  the more interesting models that  you do actually use .  by the way , in your discussion of  smiles , you discuss fat tails  extensively . at least for equity markets ,  while fat tails contribute , they don ' t do very  much at all compared to the effect  of risk premia , which you allude to briefly ,  using supply - demand language , at the  end of that section .  for the impact of risk premia , see  jun pan ' s paper , which is available  from her web page . but she covers  only sp 500 , ( as do most studies ) ,  and your markets are obviously much different .  the chapter will be a good service to your readers !  best , darrell  > x - lotus - fromdomain : ect  > from : \" vince j kaminski \"  > to : darrell duffie  > cc : \" grant masson \" , \" vince j kaminski \"  > date : wed , 16 aug 2000 16 : 37 : 53 - 0500  > subject : re : full version  > mime - version : 1 . 0  > content - disposition : inline  > x - uidl : 00453 eda 98 c 82 d 709 e 6123 af 537 e 4 f 63  > x - keywords :  >  >  >  > darrell ,  >  > thanks a lot . i really appreciate it . the text is below our usual  > standards but we are completely swamped with work here .  >  > vince  >  >  >  >  >  >  >  > darrell duffie on 08 / 15 / 2000 04 : 54 : 23 pm  >  > please respond to darrell duffie  >  > to : vince . j . kaminski @ enron . com  > cc :  > subject : re : full version  >  >  > i ' ll have a look !  >  > i haven ' t much time , but can certainly  > get you a quick reaction , at least !  >  > best , darrell  >  >  > > x - lotus - fromdomain : ect  > > from : \" vince j kaminski \"  > > to : duffie @ stanford . edu  > > date : thu , 10 aug 2000 14 : 04 : 47 - 0500  > > subject : full version  > > mime - version : 1 . 0  > > content - disposition : inline  > > x - uidl : 9 fef 7462 afa 5 d 4 ee 6 co 4 c 9 co 2 df 71 b 25  > > x - keywords :  > >  > >  > >  > > darrell ,  > >  > > grant just alerted me that i sent you only part of the text .  > >  > > here is the full chapter with an aged version of gran ' t part .  > > what i sent you represents an update of his contribution .  > >  > > sorry for that .  > >  > > vince  > >  > > ( see attached file : volo 720 . doc )  >  > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  > darrell duffie  > mail gsb stanford ca 94305 - 5015 usa  > phone 650 723 1976  > fax 650 725 7979  > email duffie @ stanford . edu  > web http : / / www . stanford . edu / ~ duffie /  > _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _  >  >  >  >  >  >  >  darrell duffie  mail gsb stanford ca 94305 - 5015 usa  phone 650 723 1976  fax 650 725 7979  email duffie @ stanford . edu  web http : / / www . stanford . edu / ~ duffie / \n",
            "spam:  [   4  341 1301  693  595  720  688  204  774  674  970  832 1058 1152\n",
            "  860  197  709 1141  824 1044  784  655   29  729 1296  344   48  477\n",
            " 1285  511 1309  537  831   43  987  515 1124  181  529 1324  413  422\n",
            "  771  785 1265  998  573  346  761 1174  474  263  454  330  792  887\n",
            "  952  958 1117  904  842  161 1131 1158  170 1273  984  605 1337 1114\n",
            "  883 1139  851  747 1012   40   60 1053 1143   16  731  707  470   80\n",
            " 1246 1112  475  449  146 1132  420  164 1182  900 1153 1140  300 1154\n",
            "  828 1358  149  647  701  805 1029 1288 1146  311 1065  502 1176 1055\n",
            "  168   97  117  869  404  549 1148 1300  854 1172  730  873 1094  862\n",
            "   38  482  193 1269  398  562  233  370 1231  838  348 1026   28   57\n",
            "  244  414  275  686  235  692 1171   37 1164 1302  368  640  591 1212\n",
            "  980  799  960  175 1077 1151  658   54   68  139  125  307  376   85\n",
            " 1024  143  121   17   87 1178  327   46 1350  123  456  433  775  762\n",
            " 1162  867 1109  694  885 1190  807  218  959  393  999  365 1216  171\n",
            "  695 1042   83 1011  406  606  766 1360  814 1030  101  708  584  645\n",
            "  245 1067  917  492  953 1316  132 1357  966  850  120  176   42   39\n",
            "  259 1208  543  202 1315  448 1033  772  392 1321 1079  955 1241  760\n",
            "  829  321   53  107  607  379 1217 1272 1021  137  579  230   79  589\n",
            "  624  719  951 1066 1003  989  225  753  282]\n",
            "spam sample= Subject: prime lenders application status  we tried to contact you last week about refinancing your home at a lower rate .  i would like to inform you know that you have been pre - approved .  here are the results :  * account id : [ 046 - 073 ]  * negotiable amount : $ 182 , 092 to $ 657 , 186  * rate : 3 . 30 % - 5 . 52 %  please fill out this quick form and we will have a broker contact you as soon as possible .  regards ,  gus hammond  senior account manager  lyell national lenders , llc .  database deletion :  www . lend - bloxz . com / r . php \n",
            "spam sample= Subject: http : / / www . blomqvist . org  hello ,  i have visited www . blomqvist . org and noticed that your website is not listed on some search engines . i am sure that through our service the number of people who visit your website will definitely increase . seekercenter is a unique technology that instantly submits your website to over 500 , 000 search engines and directories - - a really low - cost and effective way to advertise your site . for more details please go to seekercenter . net .  give your website maximum exposure today !  looking forward to hearing from you .  best regards ,  vanessa lintner  sales marketing  www . seekercenter . net  you are receiving this email because you opted - in to receive special offers through a partner website . if you feel that you received this email in error or do not wish to receive additional special offers , please enter your email address here and click the button of remove me : \n"
          ]
        }
      ]
    }
  ]
}